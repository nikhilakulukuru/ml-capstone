{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8722d0",
   "metadata": {},
   "source": [
    "This notebook demonstrates the process of getting the word embeddings matrix, which will be used in future analysis. This notebook is based on the 'CompaniesPostStemming.csv' file obtained in previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60efb3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from nltk.util import skipgrams\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de367eb3",
   "metadata": {},
   "source": [
    "# Read in the dataset and do some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = pd.read_csv('CompaniesPostStemming.csv')\n",
    "\n",
    "#Add the data of the same company to create on long string of all filings over the 2 years\n",
    "tickers = companies['ticker'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "companies2 = pd.DataFrame(columns=['ticker', 'text'])\n",
    "for i in tickers:\n",
    "    temp = companies[companies['ticker'] == i]\n",
    "    companies2.loc[count] = [i, temp['finalText'].str.cat(sep = ' ')]\n",
    "    count+=1\n",
    "    \n",
    "companies2['count'] = [len(text.split(' ')) for text in companies2['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f368987",
   "metadata": {},
   "source": [
    "# Compute PMI matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd19923",
   "metadata": {},
   "source": [
    "In order to compute numeric vectors for words, we will need to construct the PMI matrix. To do this, we first need to compute the frequency of skip-grams, which are just generalizations of n-grams that allow tokens to be skipped within some specified window. Think of it as computing the frequency of times a pair of words are observed together within some defined window indicating how far apart the tokens are allowed at max to be apart from each other.\n",
    "\n",
    "The following code shows how we can use the CountVectorizer from sklearn to count skip-grams of two words, with a window of four tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip4_bigrams = partial(skipgrams, n = 2, k = 4)\n",
    "\n",
    "def my_analyzer(text):\n",
    "    return skip4_bigrams(word_tokenize(text))\n",
    "\n",
    "skipgram_vectorizer = CountVectorizer(analyzer = my_analyzer, token_pattern = '[a-zA-Z]+')\n",
    "skipgram_count_matrix = skipgram_vectorizer.fit_transform(companies2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6509ad",
   "metadata": {},
   "source": [
    "For ease, we'll convert this into a DataFrame and see how exactly it's stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2054c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_df = pd.DataFrame(skipgram_count_matrix.toarray(),\n",
    "                           columns = skipgram_vectorizer.get_feature_names_out())\n",
    "\n",
    "#Remove all the columns with underscore inside\n",
    "def contains_underscore(x):\n",
    "    #x is a tuple of two elements, just like what the column looks like above\n",
    "    return (('_' in x[0]) or ('_' in x[1]))\n",
    "\n",
    "# Get a list of column names without underscores\n",
    "columns_to_keep = [col for col in skipgram_df.columns if not contains_underscore(col)]\n",
    "\n",
    "skipgram_df_2 = skipgram_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e2b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_long = pd.DataFrame({'counts': skipgram_df_2.sum(axis = 0).values})\n",
    "\n",
    "skipgram_long[['term1', 'term2']] = pd.DataFrame(skipgram_df_2.columns.tolist(), \n",
    "                                                index = skipgram_long.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de1ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pairs_df = skipgram_long\n",
    "del skipgram_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3bb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pairs_df['skip_p'] = full_pairs_df['counts'] / full_pairs_df['counts'].sum()\n",
    "\n",
    "unigram_vectorizer = CountVectorizer(token_pattern = '[a-zA-Z]+')\n",
    "unigram_count_matrix = unigram_vectorizer.fit_transform(companies2.text)\n",
    "unigram_long = pd.DataFrame({'uni_freq1': unigram_count_matrix.toarray().sum(axis = 0),\n",
    "                             'term1': unigram_vectorizer.get_feature_names_out()})\n",
    "unigram_long['uni_freq1'] = unigram_long['uni_freq1'] / unigram_long['uni_freq1'].sum()\n",
    "\n",
    "unigram_long2 = unigram_long.rename(columns = {\"uni_freq1\": \"uni_freq2\", \"term1\": \"term2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02981568",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pairs_df = full_pairs_df.merge(unigram_long, how = \"left\", left_on = \"term1\", right_on = \"term1\")\n",
    "full_pairs_df = full_pairs_df.merge(unigram_long2, how = \"left\", left_on = \"term2\", right_on = \"term2\")\n",
    "\n",
    "full_pairs_df['pmi'] = np.log(full_pairs_df['skip_p'] / full_pairs_df['uni_freq1'] / full_pairs_df['uni_freq2'])\n",
    "\n",
    "# Replace anything below 0 with 0, tend to focus on positive direction more than expected:\n",
    "full_pairs_df['pmi'] = np.maximum(full_pairs_df['pmi'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_wide_matrix = pd.pivot_table(full_pairs_df, index='term1', columns='term2', values='pmi',\n",
    "                                      fill_value = 0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac8c87",
   "metadata": {},
   "source": [
    "# SVD to extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bca12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_sparse = csr_matrix(skipgram_wide_matrix)\n",
    "svd = TruncatedSVD(n_components = 500, n_iter = 10, random_state = 42)\n",
    "svd_embeddings = svd.fit_transform(skipgram_sparse)\n",
    "\n",
    "word_embeddings_df = pd.DataFrame(np.transpose(svd_embeddings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
